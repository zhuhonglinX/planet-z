# 线性模型

## 基本设定

* 一对训练样本为$(x^i, y^i)$， 其中 $x^{(i)} = [x^{(1)}_{1}, x^{(1)}_{1}, \cdots ,x^{(1)}_{n}]$，表示 $x^i$ 是一个 $n$ 维向量。
* 总共有 $m$ 个训练样本，即 $\left\{ (x^1, y^1), (x^2, y^2), \cdots ,(x^m, y^m) \right\}$
* $X$ 表示全体训练集



## 假设函数 Hypothesis Function

我们的目的是通过已知的 $m$ 个训练样本，训练出一个预测函数 $h_{\theta}(x)$ ，当我们输入信息时(即输入 $x$ 向量)，该函数计算出一个值 $y_{pred}$ (预测值) ，我们希望这个预测值能够逼近 $y_{real}$ (真实值)。如果这个函数表现良好，那么当以后我们不知道真实值的时候，就可以通过预测值来充当真实值，即做出了精准的预测。



线性回归的假设函数是这样的：
$$
h_{\theta}(x) = \theta_0 + \theta_1 x_1 + \theta_2x_2 + \cdots + \theta_n x_n
$$
其中，$\theta$ 就是线性模型的参数，$\theta_0$ 称为偏置项。如果我们假定一个变量 $x_0 = 1$ ，就可以将式子改写成
$$
h_{\theta}(x) = \theta_0 x_0 + \theta_1 x_1 + \theta_2x_2 + \cdots + \theta_n x_n
$$
这样我们可以化简成向量形式：
$$
h(x) = \sum_{i=0}^{n} \theta_i x_i = \theta^T x
$$
这里重新定义 $\theta$ 和 $x$ 为向量，$\theta = (\theta_0; \theta_1; \cdots; \theta_n)$，$x = (x_0; x_1; \cdots; x_n)$，并且简化 $h_\theta(x)$ 记为 $h(x)$

 ## 代价函数 Cost Function

现在的问题就在于，如何找到一个合适的 $\theta$ 向量，能够使得 $h(x)$ 的值和结果 $y$ 尽量接近。所以现在需要一个评定方法，我们希望这个方法能够将 $h(x)$ 和 $y$ 的差别量化，因为只有数才能够直接比较大小，从而判断 $h(x)$ 的优劣。

常见得一个评定标准是均方根误差(RMSE):
$$
RMSE(\theta) = \sqrt{\frac{1}{m} \sum_{i=1}^{m} (h(x^{(i)})-y^{(i)})^2}
$$
$m$ 表示样本的数量。



实际计算的时候，根号运算比较复杂，所以使用均方误差(MSE)来代替 RMSE，这两种计算结果会得到相同的 $\theta$ 值，由此，定义代价函数 $J(\theta)$
$$
J(\theta) = \frac{1}{2}MSE(\theta) = \frac{1}{2m} \sum_{i=1}^{m}(\theta^T x^{(i)} - y^{(i)})^2
$$
由此，我们通过代价函数，成功的把 $h(x)$ 和 $y$ 的差距量化了，代价函数的输出值越小，表示这个 $h(x)$ 越合适， $\theta$ 越合适。

所以最原始的思路是代入随机的不同的 $\theta$ ，然后比较 $J(\theta)$ 值得大小，就能够知道这个 $ \theta $ 是不是合适的。但是我们不可能每次都随机 $\theta$ 然后计算代价函数，再比较大小，这样太慢了，我们需要一个方法，能够尽可能快的找到合适的 $\theta$ 值。

> 为什么要乘 $\frac{1}{2}$，这是为了之后求偏导计算方便，总之不影响最终结果。



## 正规方程 Normal Equation

首先需要介绍一点，MSE 定义的代价函数，可以证明是**凸函数** (以前高中的教科书上都叫凹函数) 。而凸函数有一个很重要的性质，即**凸函数的任何极小值就是最小值，并且凸函数只有极小值**。而我们的目的就是要求出 $J(\theta)$ 的最小值。

我们直接计算导数，在导数为 0 处，就是函数的极值，而凸函数的极值就是极小值也是最小值，因此，直接计算 $J(\theta)$ 的最小值：
$$
J(\theta) = J(\theta_0, \theta_1, \cdots, \theta_n) = \frac{1}{2m} \sum_{i=1}^{m}(\theta^T x^{(i)} - y^{(i)})^2
$$
这是一个关于 $\theta_0, \theta1, \cdots, \theta_n$ 的函数，对于每一个参数求偏导
$$
\frac{\partial}{\partial\theta_j} J(\theta) = \frac{1}{m} \sum_{i=1}^{m} [(h(x^{(i)})-y^{(i)}) x_j^{(i)}] = 0 \quad, \quad j = 0, 1, \cdots, n
$$
对于每一个 $\theta$ 都进行这个运算，然后求解。这样逐个计算参数值太麻烦了，正规方程对以上算法做出了优化，他用矩阵的形式求出 $\theta$
$$
\hat\theta = (X^TX)^{-1}X^Ty 
$$

其中 $ X = \left [ \begin{matrix} x^1 \\ x^2 \\ \vdots \\ x^m \end{matrix} \right ] $，$x^i = (x^i_0, x^i_1, \cdots, x^i_n)$， $y = (y^1, y^2, \cdots, y^m)^T$，推导过程暂时先略过。



以上完成就可以算出 $\theta$ 值，也就算出了假设函数，模型就完成了。

正规方程的计算形式很简洁，但是他也有一些缺点。正规方程需要计算矩阵 $X^TX$ 的逆，其复杂度大约是 $O(n^3)$ ，因此计算时间取决于特征数量，经验上如果特征数量小于 10000，可以考虑使用正规方程，而如果特征数量太大，就需要考虑另一种算法：梯度下降 。



## 梯度下降 Gradient Descent  

梯度下降的目的和正规方程一样，也是为了求解合适的 $\theta$ 值，最小化 $J(\theta)$。

梯度下降的思路是，对于 $J(\theta)$，是一个含有自变量 $\theta_0, \theta_1, \cdots, \theta_n$ 的函数，计算该函数的梯度：
$$
\nabla J(\theta) = (\frac{\partial J}{\partial \theta_0}, \frac{\partial J}{\partial \theta_1}, \cdots, \frac{\partial J}{\partial \theta_n})
$$
梯度的含义是，函数沿着此方向有最大的变化率，并且梯度方向是函数在给定点上升最快的方向，那么只要沿着这个梯度的反方向，就可以不断减小 $J(\theta)$，最终找到最小的值。

由此原理，梯度下降算法的实现公式如下：
$$
\theta_j = \theta_j - \alpha \frac{\partial J}{\partial \theta_j}\quad, \quad j = 0, 1, \cdots,n
$$
$\alpha$ 称为学习速率，或者步长，这个值需要手动设定：太小的话会导致收敛很慢，也就是每一次都步子都太小了，需要很长时间才能走到最小值处；太大的话会错过最小值。

另外梯度下降适合用在样本特征数量比较大的情况，而这种情况下正规方程的计算速度的表现并不好。



