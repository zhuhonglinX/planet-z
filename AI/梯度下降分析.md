# 梯度下降理解

首先，介绍泰勒展开，泰勒展开说的是：如果函数 $h(x)$ 在 $x=x_0$ 处是无限可微的，那么就有如下等式成立：
$$
h(x) = \sum^{\infin}_{k=0} \frac{h^{(k)}(x_0)}{k!}(x-x_0)^k = h(x_0) + h'(x_0)(x-x_0) + \frac {h''(x_0)}{2!}(x-x_0)^2+ \cdots
$$
函数和多项式通过泰勒的理论连接在一起了，当 $x$ 和 $x_0$ 很接近时，则可以简化为：
$$
h(x) \approx h(x_0) + h'(x_0)(x-x_0)
$$
现在假设我们有代价函数 $L(\theta)$， $\theta$ 包含 $\theta_1 \approx a$ 和 $\theta_2 \approx b$，应用到泰勒公式上（多元泰勒展开）：
$$
L(\theta) \approx L(a, b) + \frac{\partial L(a, b)}{\partial \theta_1}(\theta_1 - a) + \frac{\partial L(a, b)}{\partial \theta_2}(\theta_2 - b)
$$
对上述公式开始分析，$L(a, b)$ 其实是一个常量，我们改写为 $s$。 同理，$\frac{\partial L(a, b)}{\partial \theta_1}$ 和 $\frac{\partial L(a, b)}{\partial \theta_2}$ 表示在点 $(a, b)$ 处的偏导值，也是是常量，我们改写为 $u$ 和 $v$。另外将 $(\theta_1-a)$ 改为 $\Delta \theta_1$，$(\theta_2-b)$ 改为 $\Delta \theta_2$：
$$
L(\theta) \approx s + u \cdot \Delta \theta_1 + v \cdot \Delta \theta_2
$$
现在我们的目的是减小代价函数的值，观察发现，式子第一项是常数，无法改变，之后两项可以看成向量 $(u, v)$ 和 $(\Delta \theta_1, \Delta \theta_2)$ 的内积，于是利用内积的公式，只要两个向量方向相反，即夹角180度，就可以让内积最小，也就能够使代价函数最小。所以为了最小化代价函数，必须满足：
$$
\left[ \begin{matrix} \Delta\theta_1 \\ \Delta\theta_2 \end{matrix} \right] = - \alpha \left[ \begin{matrix} u \\ v \end{matrix} \right]
$$
因为 $\Delta\theta_1$ 和 $\Delta\theta_2$ 需要是一个很小的值，所以 $\alpha$ 应当是一个很小的值。

将公式进一步改写，常量全部代回去：
$$
\left[ \begin{matrix} \theta_1 \\ \theta_2 \end{matrix} \right] = \left[ \begin{matrix} a \\ b \end{matrix} \right] - \alpha \left[ \begin{matrix} \frac{\partial L(a, b)}{\partial \theta_1} \\ \frac{\partial L(a, b)}{\partial \theta_2} \end{matrix} \right]
$$
这就是梯度下降的公式。



# 梯度下降改进

## 随机梯度下降 SGD

后续。。。

## Adagrad 梯度下降

对于原梯度下降公式：
$$
\theta_j = \theta_j - \alpha \frac{\partial J}{\partial \theta_j}\quad, \quad j = 0, 1, \cdots,n
$$
为每一个 $\theta_j$ 设置不同的 $\alpha_j$。

首先，更新一下符号，令 $g(j) = \frac{\partial J}{\partial \theta_j}$，用上标表示更新次数（迭代次数）：

$$
\theta^1_j = \theta^0_j - \alpha \cdot g^0(j) \\
\theta^2_j = \theta^1_j - \alpha \cdot g^1(j) \\
\cdots
$$

以此类推，然后我们将每一次的梯度保存下来，求均方根：
$$
\sigma^t_j = \sqrt{\sum^{t}_{i=0}(g^i(j))^2}
$$
这里的 t 表示迭代的次数，然后写出 Adagrad 的公式：
$$
\theta^{t+1}_j = \theta^t_j - \frac {\alpha}{\sigma^t_j} \cdot g^t(j)\quad, \quad j = 0, 1, \cdots,n
$$


